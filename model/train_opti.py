import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import os
import time

# ==========================================
# 0. è®¾ç½®ä¸ç›®å½•
# ==========================================
if not os.path.exists('visual'): os.makedirs('visual')
if not os.path.exists('model'): os.makedirs('model')

sns.set(style="whitegrid", context="talk")
plt.rcParams['axes.unicode_minus'] = False 

# ==========================================
# 1. å‡†å¤‡æ•°æ®
# ==========================================
file_path = 'E:\\Machine learning\\ML-Analysis-of-F1-Fatest-Lap-Circuits\\Data_Merge\\f1_grand_dataset_full.csv'
if not os.path.exists(file_path):
    print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° CSV æ–‡ä»¶")
    exit()

df = pd.read_csv(file_path)

# ç°åœºè®¡ç®— AvgSpeed
if 'sector_length_km_S1' in df.columns and 'LapTime' in df.columns:
    total_len = df['sector_length_km_S1'] + df['sector_length_km_S2'] + df['sector_length_km_S3']
    df['AvgSpeed'] = np.where(df['LapTime'] > 0, total_len / (df['LapTime'] / 3600), 0)

# ç‰¹å¾é€‰æ‹©
features = [
    'sector_straight_ratio_S1', 'sector_straight_ratio_S2', 'sector_straight_ratio_S3',
    'sector_slow_corner_ratio_S1', 'sector_slow_corner_ratio_S2',
    'sector_length_km_S1', 'sector_length_km_S2', 'sector_length_km_S3'
]
valid_cols = [c for c in features if c in df.columns]

# æ•°æ®æ¸…æ´—
data = df[valid_cols + ['AvgSpeed']].dropna()
data = data[data['AvgSpeed'] > 0]

X = data[valid_cols]
y = data['AvgSpeed']

# åˆ‡åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"âœ… æ•°æ®å‡†å¤‡å°±ç»ªã€‚è®­ç»ƒé›†: {len(X_train)} | æµ‹è¯•é›†: {len(X_test)}")
print("="*40)

results = []

# ==========================================
# æ¨¡å‹ 1: Linear Regression (åŸºå‡†)
# ==========================================
print("1ï¸âƒ£  è®­ç»ƒ Linear Regression...")
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)
results.append({
    'Model': 'Linear Regression',
    'R2': r2_score(y_test, y_pred),
    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred))
})
# ä¿å­˜
joblib.dump(lr, 'model/linear_model.pkl')

# ==========================================
# æ¨¡å‹ 2: Random Forest (éçº¿æ€§åŸºå‡†)
# ==========================================
print("2ï¸âƒ£  è®­ç»ƒ Random Forest...")
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
results.append({
    'Model': 'Random Forest',
    'R2': r2_score(y_test, y_pred),
    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred))
})
# ä¿å­˜
joblib.dump(rf, 'model/rf_model.pkl')

# ==========================================
# æ¨¡å‹ 3: XGBoost (Grid Search ä¼˜åŒ–)
# ==========================================
print("\n3ï¸âƒ£  ğŸš€ å¯åŠ¨ XGBoost ç½‘æ ¼æœç´¢ (Grid Search)...")
print("    (æ­£åœ¨å°è¯•ä¸åŒå‚æ•°ç»„åˆï¼Œè¯·ç¨å€™...)")
start_time = time.time()

# å®šä¹‰è¦æœç´¢çš„å‚æ•°ç½‘æ ¼
param_grid = {
    'n_estimators': [100, 200, 300],     # æ ‘çš„æ•°é‡
    'learning_rate': [0.01, 0.05, 0.1],  # å­¦ä¹ ç‡
    'max_depth': [3, 5, 7],              # æ ‘çš„æ·±åº¦
    'subsample': [0.8, 1.0]              # æ ·æœ¬é‡‡æ ·
}

xgb_model = xgb.XGBRegressor(random_state=42, n_jobs=-1)

# è®¾ç½® Grid Search
grid_search = GridSearchCV(estimator=xgb_model, 
                           param_grid=param_grid, 
                           cv=3,                 # 3æŠ˜äº¤å‰éªŒè¯
                           scoring='r2',         # ä»¥ R2 ä¸ºä¼˜åŒ–ç›®æ ‡
                           n_jobs=-1, 
                           verbose=1)

# å¼€å§‹æœç´¢
grid_search.fit(X_train, y_train)

end_time = time.time()
print(f"   âœ… æœç´¢å®Œæˆï¼è€—æ—¶: {end_time - start_time:.1f}s")

# è·å–å¹¶ä¿å­˜æœ€ä½³æ¨¡å‹
best_xgb = grid_search.best_estimator_
best_params = grid_search.best_params_

print(f"   ğŸ† æœ€ä½³å‚æ•°ç»„åˆ: {best_params}")

# é¢„æµ‹
y_pred_best = best_xgb.predict(X_test)
results.append({
    'Model': 'XGBoost (Optimized)',
    'R2': r2_score(y_test, y_pred_best),
    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_best))
})

# ä¿å­˜æœ€ä½³æ¨¡å‹
joblib.dump(best_xgb, 'model/best_xgboost_model.pkl')
print("   ğŸ’¾ æœ€ä¼˜æ¨¡å‹å·²è¦†ç›–ä¿å­˜è‡³ model/best_xgboost_model.pkl")

# ==========================================
# 4. ç»“æœå¯¹æ¯”ä¸å¯è§†åŒ–
# ==========================================
results_df = pd.DataFrame(results).sort_values(by='R2', ascending=False)

print("\n=== æœ€ç»ˆå¯¹æ¯”æˆç»©å• ===")
print(results_df)

# ç”»å›¾
plt.figure(figsize=(10, 6))
colors = ['#e84d60' if 'XGBoost' in x else '#4c72b0' for x in results_df['Model']]

ax = sns.barplot(data=results_df, x='Model', y='R2', palette=colors)
plt.title('Final Model Comparison (After Optimization)', fontsize=16, weight='bold')
plt.ylim(0.9, 1.0)
plt.ylabel('RÂ² Score')
plt.xlabel('')

for i, v in enumerate(results_df['R2']):
    ax.text(i, v + 0.002, f"{v:.4f}", ha='center', fontsize=12, weight='bold')

plt.tight_layout()
plt.savefig('visual/Viz_6_Model_Comparison_Optimized.png', dpi=300)
print("\nğŸ“Š å¯¹æ¯”å›¾å·²ç”Ÿæˆ: visual/Viz_6_Model_Comparison_Optimized.png")