import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score
import os

# ==========================================
# 0. è®¾ç½®
# ==========================================
OUTPUT_FOLDER = 'visual'
if not os.path.exists(OUTPUT_FOLDER): os.makedirs(OUTPUT_FOLDER)
sns.set(style="whitegrid", context="talk")

# è¯»å–æ•°æ®
df = pd.read_csv('E:\Machine learning\ML-Analysis-of-F1-Fatest-Lap-Circuits\Data_Merge\f1_grand_dataset_full.csv')

# ç°åœºè®¡ç®— AvgSpeed
if 'sector_length_km_S1' in df.columns and 'LapTime' in df.columns:
    total_len = df['sector_length_km_S1'] + df['sector_length_km_S2'] + df['sector_length_km_S3']
    df['AvgSpeed'] = np.where(df['LapTime']>0, total_len / (df['LapTime']/3600), 0)
    
# ==========================================
# 1. å‡†å¤‡æ•°æ®
# ==========================================
features = [
    'sector_straight_ratio_S1', 'sector_straight_ratio_S2', 'sector_straight_ratio_S3',
    'sector_slow_corner_ratio_S1', 'sector_slow_corner_ratio_S2',
    'sector_length_km_S1', 'sector_length_km_S2', 'sector_length_km_S3'
]
valid_cols = [c for c in features if c in df.columns]

data = df[valid_cols + ['AvgSpeed']].dropna()
data = data[data['AvgSpeed'] > 0]

X = data[valid_cols]
y = data['AvgSpeed']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ç”¨äºå­˜å‚¨ç»“æœçš„åˆ—è¡¨
results = []

print("=== ğŸ å¼€å§‹æ¨¡å‹å¤§æ¯”æ‹¼ (Model Comparison) ===")

# ==========================================
# æ¨¡å‹ 1: Linear Regression (Baseline)
# ==========================================
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
results.append({
    'Model': 'Linear Baseline',
    'R2': r2_score(y_test, y_pred_lr),
    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_lr))
})
print(f"1. Linear Regression Done. (R2: {results[-1]['R2']:.4f})")

# ==========================================
# æ¨¡å‹ 2: Random Forest (Default)
# ==========================================
rf = RandomForestRegressor(random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
results.append({
    'Model': 'Random Forest',
    'R2': r2_score(y_test, y_pred_rf),
    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_rf))
})
print(f"2. Random Forest Done. (R2: {results[-1]['R2']:.4f})")

# ==========================================
# æ¨¡å‹ 3: XGBoost (Default)
# ==========================================
xg = xgb.XGBRegressor(random_state=42)
xg.fit(X_train, y_train)
y_pred_xg = xg.predict(X_test)
results.append({
    'Model': 'XGBoost (Default)',
    'R2': r2_score(y_test, y_pred_xg),
    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_xg))
})
print(f"3. XGBoost Default Done. (R2: {results[-1]['R2']:.4f})")

# ==========================================
# æ¨¡å‹ 4: XGBoost + Grid Search (ä½ çš„é«˜å…‰æ—¶åˆ»!)
# ==========================================
print("\nğŸ” æ­£åœ¨è¿›è¡Œç½‘æ ¼æœç´¢ (Grid Search)... è¿™å¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´...")

# å®šä¹‰è¦å°è¯•çš„å‚æ•°ç½‘æ ¼
param_grid = {
    'n_estimators': [50, 100, 200],     # å¤šå°‘æ£µæ ‘
    'learning_rate': [0.01, 0.1, 0.2],  # å­¦ä¹ ç‡
    'max_depth': [3, 5, 7],             # æ ‘çš„æ·±åº¦
    'subsample': [0.8, 1.0]             # é‡‡æ ·æ¯”ä¾‹
}

# å¯åŠ¨æœç´¢
grid_search = GridSearchCV(estimator=xgb.XGBRegressor(random_state=42),
                           param_grid=param_grid,
                           cv=3, # 3æŠ˜äº¤å‰éªŒè¯
                           n_jobs=-1, #ä»¥æ­¤ç”µè„‘å…¨åŠ›è·‘
                           verbose=0)

grid_search.fit(X_train, y_train)

# è·å–æœ€ä½³æ¨¡å‹
best_xgb = grid_search.best_estimator_
y_pred_best = best_xgb.predict(X_test)

results.append({
    'Model': 'XGBoost (Tuned)',
    'R2': r2_score(y_test, y_pred_best),
    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_best))
})

print(f"4. âœ… Grid Search å®Œæˆ!")
print(f"   æœ€ä½³å‚æ•°: {grid_search.best_params_}")
print(f"   æœ€ä½³ R2: {results[-1]['R2']:.4f}")

# ==========================================
# 5. å¯è§†åŒ–å¯¹æ¯” (ç”Ÿæˆæœ€ç»ˆç»“è®ºå›¾)
# ==========================================
results_df = pd.DataFrame(results)

# ç”»å¯¹æ¯”å›¾
plt.figure(figsize=(10, 6))
ax = sns.barplot(data=results_df, x='Model', y='R2', palette='magma')
plt.title('Final Model Comparison: RÂ² Score', fontsize=16, weight='bold')
plt.ylim(0.8, 1.0) # è®¾ç½®Yè½´èŒƒå›´ï¼Œè®©å·®å¼‚æ›´æ˜æ˜¾
plt.ylabel('RÂ² Score (Higher is Better)')
plt.xlabel('')

# åœ¨æŸ±å­ä¸Šæ ‡æ•°å€¼
for i, v in enumerate(results_df['R2']):
    ax.text(i, v + 0.005, f"{v:.4f}", ha='center', fontsize=12, weight='bold')

save_path = f"{OUTPUT_FOLDER}/Viz_6_Model_Comparison.png"
plt.savefig(save_path, dpi=300)
print(f"\nğŸ† å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")

# æ‰“å°æœ€ç»ˆè¡¨æ ¼ä¾›æŠ¥å‘Šä½¿ç”¨
print("\n=== æœ€ç»ˆæˆç»©å• (Copy to Report) ===")
print(results_df)